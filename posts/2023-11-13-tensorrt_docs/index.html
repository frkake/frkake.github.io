<!doctype html><html lang=ja dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>TensorRT Developer Guideを読んだメモ | Less is more</title><meta name=keywords content><meta name=description content="Project 2. TensorRT&rsquo;s Capabilities TensorRTには、モデルの定義とターゲットGPUへの最適化を行うビルドフェーズと、最適化されたモデルを実行する実行フェーズの2つ"><meta name=author content><link rel=canonical href=https://takumi-iida.com/posts/2023-11-13-tensorrt_docs/><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://takumi-iida.com/images/steamed_rice_brown.svg><link rel=icon type=image/png sizes=16x16 href=https://takumi-iida.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://takumi-iida.com/favicon-32x32.png><link rel=apple-touch-icon href=https://takumi-iida.com/apple-touch-icon.png><link rel=mask-icon href=https://takumi-iida.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=ja href=https://takumi-iida.com/posts/2023-11-13-tensorrt_docs/><link rel=stylesheet href=/css/biography.css><link rel=stylesheet href=/css/caption.css><link rel=stylesheet href=/css/cite.css><link rel=stylesheet href=/css/links.css><link rel=stylesheet href=/css/collapse.css><link rel=stylesheet href=/css/notice.css><link rel=stylesheet href=/css/theme-vars.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/magnific-popup.min.css><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/magnific-popup.js/1.1.0/jquery.magnific-popup.min.js></script><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><style>.katex-display{overflow-x:auto;display:inline-block;max-width:100%}</style><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css integrity=sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js integrity=sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><meta property="og:title" content="TensorRT Developer Guideを読んだメモ"><meta property="og:description" content="Project 2. TensorRT&rsquo;s Capabilities TensorRTには、モデルの定義とターゲットGPUへの最適化を行うビルドフェーズと、最適化されたモデルを実行する実行フェーズの2つ"><meta property="og:type" content="article"><meta property="og:url" content="https://takumi-iida.com/posts/2023-11-13-tensorrt_docs/"><meta property="og:image" content="https://takumi-iida.com/images/steamed_rice_brown.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-03-01T00:00:00+00:00"><meta property="article:modified_time" content="2023-11-13T12:53:11+00:00"><meta property="og:site_name" content="Less is more"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://takumi-iida.com/images/steamed_rice_brown.png"><meta name=twitter:title content="TensorRT Developer Guideを読んだメモ"><meta name=twitter:description content="Project 2. TensorRT&rsquo;s Capabilities TensorRTには、モデルの定義とターゲットGPUへの最適化を行うビルドフェーズと、最適化されたモデルを実行する実行フェーズの2つ"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://takumi-iida.com/posts/"},{"@type":"ListItem","position":2,"name":"TensorRT Developer Guideを読んだメモ","item":"https://takumi-iida.com/posts/2023-11-13-tensorrt_docs/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"TensorRT Developer Guideを読んだメモ","name":"TensorRT Developer Guideを読んだメモ","description":"Project 2. TensorRT\u0026rsquo;s Capabilities TensorRTには、モデルの定義とターゲットGPUへの最適化を行うビルドフェーズと、最適化されたモデルを実行する実行フェーズの2つ","keywords":[],"articleBody":" Project\n2. TensorRT’s Capabilities TensorRTには、モデルの定義とターゲットGPUへの最適化を行うビルドフェーズと、最適化されたモデルを実行する実行フェーズの2つのフェーズがあります。\n2.1 Build Phase Builder を使って、モデルの最適化や Engineの作成を行う。手順は次の通り。\nネットワークを定義する NetworkDefinitionインタフェースを定義する。2通りの方法がある。 ONNXからTensorRTのONNX parserを呼び出す方法 TensorRTのLayerやTensorを直接呼び出してネットワークを定義する方法の2つがある。 注意：出力としてマークされていないTensorは一時テンソルとして破棄されるので、出力したかったら名前を指定してやる必要がある。 ネットワークのconfigを指定する BuilderConfigインタフェースでTensorRTがどうやってモデルを最適化するかを指定する。 精度 (Precision)。 実行時スピードとメモリ効率とのトレードオフの制御 CUDAカーネルの選択の制約 Builderを呼び出してEngineを作る 1., 2.の情報を使って、Engineインタフェースを作る。 TensorRTのバージョンとターゲットGPUの種類によってエンジンが作成される。 TensorRTのネットワーク定義は浅いコピーなので、ビルドフェーズでメモリの開放はやらないで ビルダは一つだけ動かす。 ビルダは最速時間を計測するが、他のGPUでビルダが動いていると実行タイミングがずれるので最適化が弱くなる 2.2 Runtime Phase 実行のさせかた。最高位APIは Runtime クラス。Runtimeを使った実行のさせかたは次の通り。\nTensorRTエンジンをデシリアライズ エンジンから実行コンテクストを作成 そのとき、入出力バッファを用意する必要がある。 推論自体は enqueueV3をコールすれば実行できる。\nEngineインタフェース最適化済みモデルを持っており、はネットワークの入出力情報を提供できる。 一方で、そのEngineから作成されたExecutionContextは推論を呼び出すインタフェース。一つのエンジンに関連付けられた複数の実行コンテクストを作成して、並列実行できる。\n入出力のバッファをCPUかGPU上に用意するが、エンジンに問い合わせてどちらにバッファを用意するかを決定できる。 バッファを用意したらenqueueV3で実行できる。これにより、必要なカーネルがCUDAストリームにエンキューされ、すぐにアプリケーションの制御が戻される。 CPUとGPUの転送で時間がかかるが、こういった非同期処理を待機したい場合は cudaStreamSynchronizeを使ってストリームを同期する。\n2.3 Plugins TensorRTだけでは対応していないオペレーションの実装を提供する機構。TensorRTのPluginRegistryに登録することで、モデル変換時にONNXパーサがプラグインを利用できるようになる。 詳細\n2.4 Types and Precision TensorRTはFP32, FP16, INT8, INT32, UINT8, BOOLのデータ型に対応している。\nFP32, FP16 非量子化 INT8 暗黙的量子化 スケールファクタ(dynamic_ranges)が必要。（キャリブレーションかsetDynamicRange APIで指定） 明示的量子化 符号付き整数に解釈する。Q/DQレイヤを明示的につかってINT8型に相互変換する。 UINT8 入出力タイプにだけ利用できるデータ型 入力はUINT8からFP32 or FP16に変換される（CastLayer） 出力もCastLayerでUINT8を出力する。 量子化は非対応 ConstantLayerは出力タイプとしてはUINT8に非対応 BOOL Precisionを指定する方法は次の2つある。\nモデルレベル：BuilderFlagオプションで低精度を指定する レイヤレベル：レイヤごとに精度を指定して、数値的にセンシティブな箇所に対処する 2.5 Quantization Dynamic rangeはビルダ（キャリブレーション）かQATで計算できる。\nTODO: ここはもう少し詳しく書く\nTensorRTの量子化はSymmetric Uniform Quantization（Siggned INT8）。\n量子化前後の変換は単純な乗算で表現できる。\n量子化対象：アクティベーション、重み\nアクティベーション向けの量子化は、キャリブレーションアルゴリズムに依存する。 重み向けの量子化は、\n$$ s=\\frac{\\max \\left(\\operatorname{abs}\\left(x_{\\min }\\right), \\operatorname{abs}\\left(x_{\\max }\\right)\\right)}{127} $$\nで計算される。\n量子化\nこのスケールsが与えられたとき、量子化/逆量子化演算は $ x_q=[-128, 127] $ の整数値、$ x $をアクティベーションの浮動小数点とすると、\n$$ x_q=\\text { quantize }(x, s):=\\operatorname{roundWithTiesToEven}\\left(\\operatorname{clip}\\left(\\frac{x}{s},-128,127\\right)\\right) $$\nroundWithTiesToEvenは、最も近い偶数になる。23.5や24.5は24、-23.5や-24.5は-24になる。\nただし、OrinのDLA向けだとちょっと丸め関数が違うらしい\n$$ x_q=\\text { quantize }(x, s)=\\text { roundWithTiesToNearestEven }\\left(\\operatorname{clip}\\left(\\frac{x}{s},-128,127\\right)\\right) $$\n丸め関数 逆量子化\n$$ x=\\operatorname{dequantize}\\left(x_q, s\\right)=x_q * s $$\n量子化演算を有効にするには Builder config でINT8フラグを有効にする必要がある。\n暗黙的量子化 各量子化テンソルに紐づいたスケールを使って暗黙的な量子化や逆量子化を行う。\n暗黙的量子化の場合、TensorRTはまずグラフを最適化するときには浮動小数点モデルとして扱い、レイヤがINT8で高速になる場合にINT8で実行する。それ以外はFP32かFP16。\nAPIレベルでレイヤごとに明示的に精度を設定しても、TensorRTのグラフ最適化中に別のレイヤと融合することがあるのでレイヤごとの精度の情報が失われることがある。\nINT8が使われるかどうかが制御しづらい。\n明示的量子化 スケーリング演算を使って量子化、逆量子化が明示的に iQuantizeLayerとIDeqantizeLayerノード（Q/DQノード）によって行われる。\n明示的量子化ではINT8で量子化することを明示的に指定できる。\nImplicit vs Explicit Quantization Q/DQレイヤのあるネットワークをINT8でビルドするときはフラグを立てる必要がある。\nconfig-\u003esetFlag(BuilderFlag::kINT8); 明示的量子化では、ネットワークはINT8に双方向に型変換を行うので、INT8を型成約としてはいけない。\n重み\nQ/DQモデルの重みはFP32で指定される。IQuantizeLayerのスケールを使ってTensorRTが重みを量子化する。量子化された重みはEngineファイルに格納される。\nONNX\nPyTorchやTensorFlowからエクスポートされるONNXにはQ/DQノード（Qノードの後にDQノードが続く、Fake-Quantization）が明示的に使われることがある。\nTensorRTではそれらのQ/DQレイヤのセマンティクスを保持するので、性能劣化が少ない。（意訳）\nしかし、内部の浮動小数点演算の順序が変わる可能性があるので、ビット単位で結果が一致することはない。\nONNXのopset=10からQuantizeLinearとDequantizeLinearが追加され、TensorRTはこれをIQuantizeLayerとIDequantizeLayerにマッピングする。\nopset=13(PyTorch=1.8以降)では量子化する軸が追加され、チャネルごとの量子化ができるようになった。\n注意：ONNXのGEMM演算はチャネルごとに量子化できる。PyTorchの torch.nn.LinearレイヤはONNXでは(K, C)の重みとtransB属性（GEMM演算をする前に重みの転置を行う）を持つGEMM演算に置き換えられる。TensorFlowでは事前に転置済みの(C, K)のGEMMになる。$K=出力チャネル数, C=入力チャネル数$\nPyTorch: $ y = xW^T $ ONNX: $ y = xW $ PyTorchの重みはTensorRTで転置されるので、その重みは転置前にTensorRTによって量子化される。そのため、PyTorchからエクスポートされるONNX QATモデルは0次元目（$K=0$）でチャネルごとの量子化を行う。一方で、TensorFlowだと1次元目（$K=1$）でチャネルごとの量子化を行う。\nTensorRTは量子化済みオペレータをサポートしていない。\nつまり、ONNXの量子化済みオペレータ\nQLinearConv/ QLinearMatmul ConvInteger / MatmulInteger に遭遇したらインポートエラーを吐く。 量子化スケール 次の2種類の粒度でスケーリングできる。\nテンソルごとのスケール：単一のスケール値でテンソル全体をスケーリング チャネルごとのスケール：指定された軸にそってスケール値をブロードキャストしてスケーリング 重みはどちらかの方法でスケール、アクティベーションはテンソルごとのスケーリングのみ。\n例）重みのチャネルごとのスケーリング。2D Convカーネルの重みのshapeが KCRSでKが出力チャネル数だとすると、\n出力チャネルに対してスケーリングすることに注意する。ただし、Deconvolutionは入力チャネルに対してスケーリングする。\nfor k in range(K): for c in range(C): for r in range(R): for s in range(S): weight[k, c, r, s] = clamp(round(weight[k, c, r, s] / scale[k]), -128, 127) ↑の例で逆量子化の場合は\nfor k in range(K): for c in range(C): for r in range(R): for s in range(S): output[k, c, r, s] = input[k, c, r, s] * scale[k] Dynamic Range Dynamic rangeは量子化されたテンソルによってカバーされる範囲で、外部で求められた暗黙的な量子化に使われる。 dynamic rangeは(min, max)が設定できるが、TensorRTはSymmetric Uniform Quantizationしかサポートしていないので、 max(abs(min_float), abs(max_float)) でスケールされる（大きい方）。\nPost-Training Quantization Using Calibration (PTQ) 代表的な入力データを使って、モデル中のアクティベーションの統計情報を計算し、ベストなスケール値を求める。\n入力データは500画像くらいあれば良い。\n量子化誤差：\n離散化誤差（レンジが増えると増大する） 切り捨て誤差（レンジにより切り捨てられる値） とのバランスでスケールは求める。そのため、TensorRTにはキャリブレータがいくつかある。\nkCALIBRATE_BEFORE_FUSION: キャリブレーション前に性能に影響のないレイヤフュージョンを行うキャリブレータ。しかし、DLAを使う場合に問題ある。\nキャリブレーションのバッチサイズは、IInt8EntropyCalibrator2 と IInt8EntropyCalibrator の切り捨て誤差に影響がでる。\n小さなキャリブレーションバッチはヒストグラムの解像度が低下し、スケールの精度が低下する。アクティベーションの値がヒストグラムの最大値よりも高いと、ヒストグラムの範囲は2のべき乗ごとに増える。\n最終のキャリブレーションステップで再割り当てがおきてヒストグラムのビンの半分（片側）が空になる場合以外は良い結果になる。キャリブレーションバッチの順序に影響を受ける。（最後出なかったら切り捨てられることもあるから？）\nなので、できるだけバッチサイズを大きくしたほうが良い。\nキャリブレータ\nIInt8EntropyCalibrator2: 量子化後の情報理論に基づいたエントロピーキャリブレーション。外れ値は除去される。DLAに必要。デフォルトでは、キャリブレーションはレイヤフュージョン前に行われる。CNN向け。 IInt8MinMaxCalibrator: アクティベーションのフルレンジを利用。デフォルトでは、キャリブレーションはレイヤフュージョン前に行われる。NLP向け。BERT向け。 IInt8EntropyCalibrator: LegacyCalibratorよりもシンプルで良い結果が出る。デフォルトでは、キャリブレーションはレイヤフュージョン後に行われる。 IInt8LegacyCalibrator: TensorRT 2.0 EAと互換性のあるキャリブレーション。ユーザのパラメータ化や必要で悪い結果の場合フォールバックされる。デフォルトでは、キャリブレーションはレイヤフュージョン後に行われる。パーセンタイルでキャリブレーションできる (99.99%パーセンタイル)。BERTやNeMo ASR model QuartzNetなどで良い結果だった（昔は？）。 手順としては、\n32ビットエンジンをビルドして、アクティベーションのヒストグラムを作ってキャリブレーションする。 ヒストグラムをもとに各テンソルのキャリブレーションテーブル（スケール値）を作る。 INT8エンジンをキャリブレーションテーブルをもとにビルドする。 キャリブレーションは遅いので、ステップ2のテーブルは保存しておいた方が良い。\nレイヤフュージョン前のキャリブレーションだったらデバイス間で移植可能。\nつまり、IInt8EntropyCalibrator2やIInt8MinMaxCalibratorを使うときやQuantizationFlag::kCALIBRATE_BEFORE_FUSIONフラグがセットされているときは移植できる。 レイヤフュージョンはプラットフォームやデバイスによって動作が異なることがあるので、フュージョン後だとキャリブレーションキャッシュが利用できないかもしれない。\nNote: ビルダがINT8の入出力を使うと設定されていても、TensorRTはキャリブレーションデータはFP32であることを想定している（入出力はFP32）。このときはINT8のI/OをFP32にキャスト（[128.0F, 127.0F]の範囲）する必要がある。\nNote: キャリブレーションは決定的で、TensorRTに同じデータ、同じ順序、同じデバイスで入力されたら、同じスケール値が出る。\n2.7 Dynamic shape TensorRTは入力形状に基づいてモデルを最適化するが、実行時に動的な形状をサポートしている。OptimizationProfile最小、最大入力形状を指定する。\nTODO: ここはもう少し詳しく書く\n2.8 DLA TODO: ここはもう少し詳しく書く\n2.10 trtexec ランダム値 or ユーザ指定のデータを使ったネットワークベンチマーク モデルのエンジン化（シリアライズ化） ビルダからシリアライズ化されたタイミングキャッシュを作成 2.11 Polygraphy TensorRTモデルの実行やデバッグをするためのツール。\n複数のバックエンドで実行する (TensorRT, ONNX-runtime) モデルを複数フォーマットに変換する。 e.x. TensorRT engine with post-training quantization モデルの様々なタイプの情報表示 ONNXモデルをコマンドラインから変更 サブグラフの抽出 単純化やサニタイズ化 (simplify and sanitize) I/O Formats TODO: ここはもう少し詳しく書く\n","wordCount":"5621","inLanguage":"ja","datePublished":"2023-03-01T00:00:00Z","dateModified":"2023-11-13T12:53:11Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://takumi-iida.com/posts/2023-11-13-tensorrt_docs/"},"publisher":{"@type":"Organization","name":"Less is more","logo":{"@type":"ImageObject","url":"https://takumi-iida.com/images/steamed_rice_brown.svg"}}}</script><script>document.addEventListener("DOMContentLoaded",function(){var e=document.querySelectorAll(".link-item a i");e.forEach(function(e){e.parentNode.style.textDecoration="none",e.parentNode.style.borderBottom="none",e.parentNode.style.outline="none"})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-G70HSBBZFQ"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-G70HSBBZFQ")</script></head><body id=top><link rel=stylesheet href=/css/pubdate.css><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://takumi-iida.com/ accesskey=h title="Less is more (Alt + H)"><img src=https://takumi-iida.com/images/steamed_rice_brown.svg alt aria-label=logo height=30>Less is more</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://takumi-iida.com/en/ title=English aria-label=English>En</a></li></ul></div></div><ul id=menu><li><a href=https://takumi-iida.com/aboutme title=プロフィール><span>プロフィール</span></a></li><li><a href=https://takumi-iida.com/posts title=記事><span>記事</span></a></li><li><a href=https://takumi-iida.com/tags title=タグ><span>タグ</span></a></li><li><a href=https://takumi-iida.com/pubdate title=ソート><span>ソート</span></a></li><li><a href=https://takumi-iida.com/search title=検索><span>検索</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>TensorRT Developer Guideを読んだメモ</h1><div class=post-meta><span title='2023-11-13 12:53:11 +0000 UTC'>2023-11-13</span></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>目次</span></summary><div class=inner><ul><li><a href=#2-tensorrts-capabilities aria-label="2. TensorRT&amp;rsquo;s Capabilities">2. TensorRT&rsquo;s Capabilities</a><ul><li><a href=#21-build-phase aria-label="2.1 Build Phase">2.1 Build Phase</a></li><li><a href=#22-runtime-phase aria-label="2.2 Runtime Phase">2.2 Runtime Phase</a></li><li><a href=#23-plugins aria-label="2.3 Plugins">2.3 Plugins</a></li><li><a href=#24-types-and-precision aria-label="2.4 Types and Precision">2.4 Types and Precision</a></li><li><a href=#25-quantization aria-label="2.5 Quantization">2.5 Quantization</a><ul><li><a href=#%e6%9a%97%e9%bb%99%e7%9a%84%e9%87%8f%e5%ad%90%e5%8c%96 aria-label=暗黙的量子化>暗黙的量子化</a></li><li><a href=#%e6%98%8e%e7%a4%ba%e7%9a%84%e9%87%8f%e5%ad%90%e5%8c%96 aria-label=明示的量子化>明示的量子化</a></li><li><a href=#%e9%87%8f%e5%ad%90%e5%8c%96%e3%82%b9%e3%82%b1%e3%83%bc%e3%83%ab aria-label=量子化スケール>量子化スケール</a></li><li><a href=#dynamic-range aria-label="Dynamic Range">Dynamic Range</a></li><li><a href=#post-training-quantization-using-calibration-ptq aria-label="Post-Training Quantization Using Calibration (PTQ)">Post-Training Quantization Using Calibration (PTQ)</a></li></ul></li><li><a href=#27-dynamic-shape aria-label="2.7 Dynamic shape">2.7 Dynamic shape</a></li><li><a href=#28-dla aria-label="2.8 DLA">2.8 DLA</a></li><li><a href=#210-trtexec aria-label="2.10 trtexec">2.10 <code>trtexec</code></a></li><li><a href=#211-polygraphy aria-label="2.11 Polygraphy">2.11 Polygraphy</a></li><li><a href=#io-formats aria-label="I/O Formats">I/O Formats</a></li></ul></li></ul></div></details></div><div class=post-content><div class=links-grid><div class=link-item><a href=https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/ target=_blank><i class="fas fa-project-diagram"></i><p>Project</p></a></div><div class="link-item hidden"></div><div class="link-item hidden"></div><div class="link-item hidden"></div></div><hr><h2 id=2-tensorrts-capabilities>2. TensorRT&rsquo;s Capabilities<a hidden class=anchor aria-hidden=true href=#2-tensorrts-capabilities>#</a></h2><p>TensorRTには、モデルの定義とターゲットGPUへの最適化を行うビルドフェーズと、最適化されたモデルを実行する実行フェーズの2つのフェーズがあります。</p><h3 id=21-build-phase>2.1 Build Phase<a hidden class=anchor aria-hidden=true href=#21-build-phase>#</a></h3><p><code>Builder</code> を使って、モデルの最適化や <code>Engine</code>の作成を行う。手順は次の通り。</p><ol><li>ネットワークを定義する<ul><li><code>NetworkDefinition</code>インタフェースを定義する。2通りの方法がある。<ul><li>ONNXからTensorRTのONNX parserを呼び出す方法</li><li>TensorRTの<code>Layer</code>や<code>Tensor</code>を直接呼び出してネットワークを定義する方法の2つがある。</li></ul></li><li>注意：出力としてマークされていないTensorは一時テンソルとして破棄されるので、出力したかったら名前を指定してやる必要がある。</li></ul></li><li>ネットワークのconfigを指定する<ul><li><code>BuilderConfig</code>インタフェースでTensorRTがどうやってモデルを最適化するかを指定する。<ul><li>精度 (Precision)。</li><li>実行時スピードとメモリ効率とのトレードオフの制御</li><li>CUDAカーネルの選択の制約</li></ul></li></ul></li><li>Builderを呼び出してEngineを作る<ul><li>1., 2.の情報を使って、<code>Engine</code>インタフェースを作る。</li><li>TensorRTのバージョンとターゲットGPUの種類によってエンジンが作成される。</li><li>TensorRTのネットワーク定義は浅いコピーなので、ビルドフェーズでメモリの開放はやらないで</li><li>ビルダは一つだけ動かす。<ul><li>ビルダは最速時間を計測するが、他のGPUでビルダが動いていると実行タイミングがずれるので最適化が弱くなる</li></ul></li></ul></li></ol><h3 id=22-runtime-phase>2.2 Runtime Phase<a hidden class=anchor aria-hidden=true href=#22-runtime-phase>#</a></h3><p>実行のさせかた。最高位APIは <code>Runtime</code> クラス。<code>Runtime</code>を使った実行のさせかたは次の通り。</p><ol><li>TensorRTエンジンをデシリアライズ</li><li>エンジンから実行コンテクストを作成</li></ol><p>そのとき、入出力バッファを用意する必要がある。
推論自体は <code>enqueueV3</code>をコールすれば実行できる。</p><p><code>Engine</code>インタフェース最適化済みモデルを持っており、はネットワークの入出力情報を提供できる。
一方で、その<code>Engine</code>から作成された<code>ExecutionContext</code>は推論を呼び出すインタフェース。一つのエンジンに関連付けられた複数の実行コンテクストを作成して、並列実行できる。</p><p>入出力のバッファをCPUかGPU上に用意するが、エンジンに問い合わせてどちらにバッファを用意するかを決定できる。
バッファを用意したら<code>enqueueV3</code>で実行できる。これにより、必要なカーネルがCUDAストリームにエンキューされ、すぐにアプリケーションの制御が戻される。
CPUとGPUの転送で時間がかかるが、こういった非同期処理を待機したい場合は <code>cudaStreamSynchronize</code>を使ってストリームを同期する。</p><h3 id=23-plugins>2.3 Plugins<a hidden class=anchor aria-hidden=true href=#23-plugins>#</a></h3><p>TensorRTだけでは対応していないオペレーションの実装を提供する機構。TensorRTの<code>PluginRegistry</code>に登録することで、モデル変換時にONNXパーサがプラグインを利用できるようになる。
<a href=https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#extending>詳細</a></p><h3 id=24-types-and-precision>2.4 Types and Precision<a hidden class=anchor aria-hidden=true href=#24-types-and-precision>#</a></h3><p>TensorRTはFP32, FP16, INT8, INT32, UINT8, BOOLのデータ型に対応している。</p><ul><li>FP32, FP16<ul><li>非量子化</li></ul></li><li>INT8<ul><li>暗黙的量子化<ul><li>スケールファクタ(dynamic_ranges)が必要。（キャリブレーションか<code>setDynamicRange</code> APIで指定）</li></ul></li><li>明示的量子化<ul><li>符号付き整数に解釈する。Q/DQレイヤを明示的につかってINT8型に相互変換する。</li></ul></li></ul></li><li>UINT8<ul><li>入出力タイプにだけ利用できるデータ型</li><li>入力はUINT8からFP32 or FP16に変換される（<code>CastLayer</code>）</li><li>出力も<code>CastLayer</code>でUINT8を出力する。</li><li>量子化は非対応</li><li><code>ConstantLayer</code>は出力タイプとしてはUINT8に非対応</li></ul></li><li>BOOL</li></ul><p>Precisionを指定する方法は次の2つある。</p><ul><li>モデルレベル：<code>BuilderFlag</code>オプションで低精度を指定する</li><li>レイヤレベル：レイヤごとに精度を指定して、数値的にセンシティブな箇所に対処する</li></ul><h3 id=25-quantization>2.5 Quantization<a hidden class=anchor aria-hidden=true href=#25-quantization>#</a></h3><p>Dynamic rangeはビルダ（キャリブレーション）かQATで計算できる。</p><p>TODO: <a href=https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#working-with-int8>ここはもう少し詳しく書く</a></p><p>TensorRTの量子化はSymmetric Uniform Quantization（Siggned INT8）。<br>量子化前後の変換は単純な乗算で表現できる。</p><p>量子化対象：アクティベーション、重み<br>アクティベーション向けの量子化は、キャリブレーションアルゴリズムに依存する。
重み向けの量子化は、</p><p>$$
s=\frac{\max \left(\operatorname{abs}\left(x_{\min }\right), \operatorname{abs}\left(x_{\max }\right)\right)}{127}
$$</p><p>で計算される。</p><p><strong>量子化</strong></p><p>このスケールsが与えられたとき、量子化/逆量子化演算は $ x_q=[-128, 127] $ の整数値、$ x $をアクティベーションの浮動小数点とすると、</p><p>$$
x_q=\text { quantize }(x, s):=\operatorname{roundWithTiesToEven}\left(\operatorname{clip}\left(\frac{x}{s},-128,127\right)\right)
$$</p><p><code>roundWithTiesToEven</code>は、最も近い偶数になる。23.5や24.5は24、-23.5や-24.5は-24になる。</p><p>ただし、OrinのDLA向けだとちょっと丸め関数が違うらしい</p><p>$$
x_q=\text { quantize }(x, s)=\text { roundWithTiesToNearestEven }\left(\operatorname{clip}\left(\frac{x}{s},-128,127\right)\right)
$$</p><p><figure class=centered-figure><a href=image-1.png class=magnific-image><img src=image-1.png alt=丸め関数></a><figcaption>丸め関数</figcaption></figure></p><p><strong>逆量子化</strong></p><p>$$
x=\operatorname{dequantize}\left(x_q, s\right)=x_q * s
$$</p><p>量子化演算を有効にするには Builder config でINT8フラグを有効にする必要がある。</p><h4 id=暗黙的量子化>暗黙的量子化<a hidden class=anchor aria-hidden=true href=#暗黙的量子化>#</a></h4><p>各量子化テンソルに紐づいたスケールを使って暗黙的な量子化や逆量子化を行う。</p><p>暗黙的量子化の場合、TensorRTはまずグラフを最適化するときには浮動小数点モデルとして扱い、レイヤがINT8で高速になる場合にINT8で実行する。それ以外はFP32かFP16。<br>APIレベルでレイヤごとに明示的に精度を設定しても、TensorRTのグラフ最適化中に別のレイヤと融合することがあるのでレイヤごとの精度の情報が失われることがある。<br>INT8が使われるかどうかが制御しづらい。</p><h4 id=明示的量子化>明示的量子化<a hidden class=anchor aria-hidden=true href=#明示的量子化>#</a></h4><p>スケーリング演算を使って量子化、逆量子化が明示的に <code>iQuantizeLayer</code>と<code>IDeqantizeLayer</code>ノード（Q/DQノード）によって行われる。<br>明示的量子化ではINT8で量子化することを明示的に指定できる。</p><p><figure class=centered-figure><a href=image.png class=magnific-image><img src=image.png alt="Implicit vs Explicit Quantization"></a><figcaption>Implicit vs Explicit Quantization</figcaption></figure></p><p>Q/DQレイヤのあるネットワークをINT8でビルドするときはフラグを立てる必要がある。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=display:flex><span>config<span style=color:#f92672>-&gt;</span>setFlag(BuilderFlag<span style=color:#f92672>::</span>kINT8);
</span></span></code></pre></div><p>明示的量子化では、ネットワークはINT8に双方向に型変換を行うので、INT8を型成約としてはいけない。</p><p><strong>重み</strong></p><p>Q/DQモデルの重みはFP32で指定される。<code>IQuantizeLayer</code>のスケールを使ってTensorRTが重みを量子化する。量子化された重みは<code>Engine</code>ファイルに格納される。</p><p><strong>ONNX</strong></p><p>PyTorchやTensorFlowからエクスポートされるONNXにはQ/DQノード（Qノードの後にDQノードが続く、Fake-Quantization）が明示的に使われることがある。<br>TensorRTではそれらのQ/DQレイヤのセマンティクスを保持するので、性能劣化が少ない。（意訳）<br>しかし、内部の浮動小数点演算の順序が変わる可能性があるので、ビット単位で結果が一致することはない。</p><p>ONNXのopset=10から<code>QuantizeLinear</code>と<code>DequantizeLinear</code>が追加され、TensorRTはこれを<code>IQuantizeLayer</code>と<code>IDequantizeLayer</code>にマッピングする。<br>opset=13(PyTorch=1.8以降)では量子化する軸が追加され、チャネルごとの量子化ができるようになった。</p><p>注意：ONNXのGEMM演算はチャネルごとに量子化できる。PyTorchの <code>torch.nn.Linear</code>レイヤはONNXでは(K, C)の重みと<code>transB</code>属性（GEMM演算をする前に重みの転置を行う）を持つGEMM演算に置き換えられる。TensorFlowでは事前に転置済みの(C, K)のGEMMになる。$K=出力チャネル数, C=入力チャネル数$</p><ul><li>PyTorch: $ y = xW^T $</li><li>ONNX: $ y = xW $</li></ul><p>PyTorchの重みはTensorRTで転置されるので、その重みは転置前にTensorRTによって量子化される。そのため、PyTorchからエクスポートされるONNX QATモデルは0次元目（$K=0$）でチャネルごとの量子化を行う。一方で、TensorFlowだと1次元目（$K=1$）でチャネルごとの量子化を行う。</p><p>TensorRTは量子化済みオペレータをサポートしていない。<br>つまり、ONNXの量子化済みオペレータ</p><ul><li><code>QLinearConv</code>/ <code>QLinearMatmul</code></li><li><code>ConvInteger</code> / <code>MatmulInteger</code>
に遭遇したらインポートエラーを吐く。</li></ul><h4 id=量子化スケール>量子化スケール<a hidden class=anchor aria-hidden=true href=#量子化スケール>#</a></h4><p>次の2種類の粒度でスケーリングできる。</p><ul><li>テンソルごとのスケール：単一のスケール値でテンソル全体をスケーリング</li><li>チャネルごとのスケール：指定された軸にそってスケール値をブロードキャストしてスケーリング</li></ul><p>重みはどちらかの方法でスケール、アクティベーションはテンソルごとのスケーリングのみ。</p><p>例）重みのチャネルごとのスケーリング。2D Convカーネルの重みのshapeが <code>KCRS</code>で<code>K</code>が出力チャネル数だとすると、<br>出力チャネルに対してスケーリングすることに注意する。ただし、Deconvolutionは入力チャネルに対してスケーリングする。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(K):
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> range(C):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> r <span style=color:#f92672>in</span> range(R):
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>for</span> s <span style=color:#f92672>in</span> range(S):
</span></span><span style=display:flex><span>        weight[k, c, r, s] <span style=color:#f92672>=</span> clamp(round(weight[k, c, r, s] <span style=color:#f92672>/</span> scale[k]), <span style=color:#f92672>-</span><span style=color:#ae81ff>128</span>, <span style=color:#ae81ff>127</span>)
</span></span></code></pre></div><p>↑の例で逆量子化の場合は</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(K):
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>for</span> c <span style=color:#f92672>in</span> range(C):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> r <span style=color:#f92672>in</span> range(R):
</span></span><span style=display:flex><span>      <span style=color:#66d9ef>for</span> s <span style=color:#f92672>in</span> range(S):
</span></span><span style=display:flex><span>        output[k, c, r, s] <span style=color:#f92672>=</span> input[k, c, r, s] <span style=color:#f92672>*</span> scale[k]
</span></span></code></pre></div><h4 id=dynamic-range>Dynamic Range<a hidden class=anchor aria-hidden=true href=#dynamic-range>#</a></h4><p>Dynamic rangeは量子化されたテンソルによってカバーされる範囲で、外部で求められた暗黙的な量子化に使われる。
dynamic rangeは(min, max)が設定できるが、TensorRTはSymmetric Uniform Quantizationしかサポートしていないので、 <code>max(abs(min_float), abs(max_float))</code> でスケールされる（大きい方）。</p><h4 id=post-training-quantization-using-calibration-ptq>Post-Training Quantization Using Calibration (PTQ)<a hidden class=anchor aria-hidden=true href=#post-training-quantization-using-calibration-ptq>#</a></h4><p>代表的な入力データを使って、モデル中のアクティベーションの統計情報を計算し、ベストなスケール値を求める。<br>入力データは500画像くらいあれば良い。</p><p>量子化誤差：</p><ul><li>離散化誤差（レンジが増えると増大する）</li><li>切り捨て誤差（レンジにより切り捨てられる値）</li></ul><p>とのバランスでスケールは求める。そのため、TensorRTにはキャリブレータがいくつかある。</p><p><code>kCALIBRATE_BEFORE_FUSION</code>: キャリブレーション前に性能に影響のないレイヤフュージョンを行うキャリブレータ。しかし、DLAを使う場合に問題ある。<br>キャリブレーションのバッチサイズは、<code>IInt8EntropyCalibrator2</code> と <code>IInt8EntropyCalibrator</code> の切り捨て誤差に影響がでる。<br>小さなキャリブレーションバッチはヒストグラムの解像度が低下し、スケールの精度が低下する。アクティベーションの値がヒストグラムの最大値よりも高いと、ヒストグラムの範囲は2のべき乗ごとに増える。<br>最終のキャリブレーションステップで再割り当てがおきてヒストグラムのビンの半分（片側）が空になる場合以外は良い結果になる。キャリブレーションバッチの順序に影響を受ける。（最後出なかったら切り捨てられることもあるから？）<br>なので、できるだけバッチサイズを大きくしたほうが良い。</p><p><strong>キャリブレータ</strong></p><ul><li><code>IInt8EntropyCalibrator2</code>: 量子化後の情報理論に基づいたエントロピーキャリブレーション。外れ値は除去される。DLAに必要。デフォルトでは、キャリブレーションはレイヤフュージョン前に行われる。CNN向け。</li><li><code>IInt8MinMaxCalibrator</code>: アクティベーションのフルレンジを利用。デフォルトでは、キャリブレーションはレイヤフュージョン前に行われる。NLP向け。BERT向け。</li><li><code>IInt8EntropyCalibrator</code>: <code>LegacyCalibrator</code>よりもシンプルで良い結果が出る。デフォルトでは、キャリブレーションはレイヤフュージョン後に行われる。</li><li><code>IInt8LegacyCalibrator</code>: TensorRT 2.0 EAと互換性のあるキャリブレーション。ユーザのパラメータ化や必要で悪い結果の場合フォールバックされる。デフォルトでは、キャリブレーションはレイヤフュージョン後に行われる。パーセンタイルでキャリブレーションできる (99.99%パーセンタイル)。BERTやNeMo ASR model QuartzNetなどで良い結果だった（昔は？）。</li></ul><p>手順としては、</p><ol><li>32ビットエンジンをビルドして、アクティベーションのヒストグラムを作ってキャリブレーションする。</li><li>ヒストグラムをもとに各テンソルのキャリブレーションテーブル（スケール値）を作る。</li><li>INT8エンジンをキャリブレーションテーブルをもとにビルドする。</li></ol><p>キャリブレーションは遅いので、ステップ2のテーブルは保存しておいた方が良い。<br>レイヤフュージョン前のキャリブレーションだったらデバイス間で移植可能。<br>つまり、<code>IInt8EntropyCalibrator2</code>や<code>IInt8MinMaxCalibrator</code>を使うときや<code>QuantizationFlag::kCALIBRATE_BEFORE_FUSION</code>フラグがセットされているときは移植できる。
レイヤフュージョンはプラットフォームやデバイスによって動作が異なることがあるので、フュージョン後だとキャリブレーションキャッシュが利用できないかもしれない。</p><p>Note: ビルダがINT8の入出力を使うと設定されていても、TensorRTはキャリブレーションデータはFP32であることを想定している（入出力はFP32）。このときはINT8のI/OをFP32にキャスト（[128.0F, 127.0F]の範囲）する必要がある。</p><p>Note: キャリブレーションは決定的で、TensorRTに同じデータ、同じ順序、同じデバイスで入力されたら、同じスケール値が出る。</p><h3 id=27-dynamic-shape>2.7 Dynamic shape<a hidden class=anchor aria-hidden=true href=#27-dynamic-shape>#</a></h3><p>TensorRTは入力形状に基づいてモデルを最適化するが、実行時に動的な形状をサポートしている。<code>OptimizationProfile</code>最小、最大入力形状を指定する。</p><p>TODO: <a href=https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#work_dynamic_shapes>ここはもう少し詳しく書く</a></p><h3 id=28-dla>2.8 DLA<a hidden class=anchor aria-hidden=true href=#28-dla>#</a></h3><p>TODO: <a href=https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#dla_topic>ここはもう少し詳しく書く</a></p><h3 id=210-trtexec>2.10 <code>trtexec</code><a hidden class=anchor aria-hidden=true href=#210-trtexec>#</a></h3><ul><li>ランダム値 or ユーザ指定のデータを使ったネットワークベンチマーク</li><li>モデルのエンジン化（シリアライズ化）</li><li>ビルダからシリアライズ化されたタイミングキャッシュを作成</li></ul><h3 id=211-polygraphy>2.11 Polygraphy<a hidden class=anchor aria-hidden=true href=#211-polygraphy>#</a></h3><p>TensorRTモデルの実行やデバッグをするためのツール。</p><ul><li>複数のバックエンドで実行する (TensorRT, ONNX-runtime)</li><li>モデルを複数フォーマットに変換する。<ul><li>e.x. TensorRT engine with post-training quantization</li></ul></li><li>モデルの様々なタイプの情報表示</li><li>ONNXモデルをコマンドラインから変更<ul><li>サブグラフの抽出</li><li>単純化やサニタイズ化 (simplify and sanitize)</li></ul></li></ul><h3 id=io-formats>I/O Formats<a hidden class=anchor aria-hidden=true href=#io-formats>#</a></h3><p>TODO: <a href=https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#reformat-free-network-tensors>ここはもう少し詳しく書く</a></p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://takumi-iida.com/posts/2023-11-14-3d-gaussian-splatting/><span class=title>« 前へ</span><br><span>3D Gaussian Splatting</span></a>
<a class=next href=https://takumi-iida.com/posts/2023-10-01-photoguard/><span class=title>次へ »</span><br><span>PhotoGuard</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share TensorRT Developer Guideを読んだメモ on twitter" href="https://twitter.com/intent/tweet/?text=TensorRT%20Developer%20Guide%e3%82%92%e8%aa%ad%e3%82%93%e3%81%a0%e3%83%a1%e3%83%a2&amp;url=https%3a%2f%2ftakumi-iida.com%2fposts%2f2023-11-13-tensorrt_docs%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share TensorRT Developer Guideを読んだメモ on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftakumi-iida.com%2fposts%2f2023-11-13-tensorrt_docs%2f&amp;title=TensorRT%20Developer%20Guide%e3%82%92%e8%aa%ad%e3%82%93%e3%81%a0%e3%83%a1%e3%83%a2&amp;summary=TensorRT%20Developer%20Guide%e3%82%92%e8%aa%ad%e3%82%93%e3%81%a0%e3%83%a1%e3%83%a2&amp;source=https%3a%2f%2ftakumi-iida.com%2fposts%2f2023-11-13-tensorrt_docs%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share TensorRT Developer Guideを読んだメモ on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftakumi-iida.com%2fposts%2f2023-11-13-tensorrt_docs%2f&title=TensorRT%20Developer%20Guide%e3%82%92%e8%aa%ad%e3%82%93%e3%81%a0%e3%83%a1%e3%83%a2"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share TensorRT Developer Guideを読んだメモ on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftakumi-iida.com%2fposts%2f2023-11-13-tensorrt_docs%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></div></footer></article></main><script src=/js/cite.js></script>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="コピー";function s(){t.innerHTML="コピーされました!",setTimeout(()=>{t.innerHTML="コピー"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script><script>$(document).ready(function(){$(".magnific-image").magnificPopup({type:"image",closeOnContentClick:!0})})</script></body></html>